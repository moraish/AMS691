{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "01e22de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "print(client.is_ready())  # Should print: `True`\n",
    "\n",
    "# client.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e6c3e4",
   "metadata": {},
   "source": [
    "## 1. Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead2443b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'ResearchPapers' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/weaviate/collections/classes/config.py:1950: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n",
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "/var/folders/86/3b88347n3ts9zy55rbjcw_b80000gn/T/ipykernel_1653/2613670611.py:10: ResourceWarning: unclosed <socket.socket fd=106, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 50505, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  papers = client.collections.create(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# client.collections.delete(\"ResearchPapers\")  # THIS WILL DELETE THE SPECIFIED COLLECTION(S) AND THEIR OBJECTS\n",
    "\n",
    "try:\n",
    "    papers = client.collections.create(\n",
    "        name=\"ResearchPapers\",\n",
    "        vectorizer_config=Configure.Vectorizer.text2vec_ollama(     # Configure the Ollama embedding integration\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "            model=\"nomic-embed-text\",                               # The model to use\n",
    "        ),\n",
    "        generative_config=Configure.Generative.ollama(              # Configure the Ollama generative integration\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "            model=\"llama3.2\",                                       # The model to use\n",
    "        ),\n",
    "        properties=[\n",
    "            Property(name=\"paper_title\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "            Property(name=\"doc_id\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "            Property(name=\"chunk_text\", data_type=DataType.TEXT, skip_vectorization=False),\n",
    "            Property(name=\"tag\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Collection 'ResearchPapers' created successfully.\")\n",
    "except weaviate.exceptions.WeaviateQueryError as e:\n",
    "    print(f\"Error creating collection: {e}\")\n",
    "    # Optionally, handle the error or exit\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    # Handle other exceptions\n",
    "finally:\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a7b32",
   "metadata": {},
   "source": [
    "## 2. Ingest Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d3094591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import SentenceTransformersTokenTextSplitter\n",
    "# from unstructured.partition.auto import partition\n",
    "\n",
    "# Define paths\n",
    "pdf_path = '/Users/moraish/Desktop/ams691/project_llm/data/1_Prefix-Tuning- Optimizing Continuous Prompts for Generation.pdf'\n",
    "\n",
    "def get_chunks(document, chunk_size=1000, chunk_overlap=200):\n",
    "    splitter = SentenceTransformersTokenTextSplitter(\n",
    "        chunk_size=chunk_size, \n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    \n",
    "    if isinstance(document, str):\n",
    "        return splitter.split_text(document)\n",
    "    elif isinstance(document, list):\n",
    "        # Assume document is a list of Document objects with a \"page_content\" attribute\n",
    "        chunks = splitter.split_documents(document)\n",
    "        return [chunk.page_content for chunk in chunks]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported document type: Expected str or list of Document objects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "bcbbdc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(filename, chunks):\n",
    "    base = os.path.basename(filename)\n",
    "    # Remove the file extension and split into paper id and title\n",
    "    if not base.lower().endswith(\".pdf\"):\n",
    "        raise ValueError(\"Filename must be a PDF.\")\n",
    "    \n",
    "    name_without_ext = base[:-4]\n",
    "    # Split at first underscore to get id and title\n",
    "    parts = name_without_ext.split(\"_\", 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(\"Filename does not match the expected format '{paper_id}_{paper_title}.pdf'\")\n",
    "    \n",
    "    paper_id, paper_title = parts[0], parts[1]\n",
    "    \n",
    "    # Generate a metadata object for each chunk.\n",
    "    documents = []\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        doc_id = f\"{paper_id}_chunk_{index+1}\"\n",
    "        documents.append({\n",
    "            \"paper_title\": paper_title,\n",
    "            \"doc_id\": doc_id,\n",
    "            \"chunk_text\": chunk,\n",
    "            \"tag\": \"\"\n",
    "        })\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "86bbf88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/pypdf/generic/_base.py:869: ResourceWarning: unclosed <socket.socket fd=97, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 50068, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  return NameObject(ret)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/pypdf/generic/_base.py:869: ResourceWarning: unclosed <socket.socket fd=98, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 50362, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  return NameObject(ret)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/pypdf/generic/_base.py:869: ResourceWarning: unclosed <socket.socket fd=100, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 50381, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  return NameObject(ret)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/pypdf/generic/_base.py:869: ResourceWarning: unclosed <socket.socket fd=101, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 50427, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  return NameObject(ret)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 32 0 (offset 0)\n",
      "Ignoring wrong pointing object 89 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Assume get_chunks and process_document functions are defined in previous cells\n",
    "\n",
    "data_directory = \"/Users/moraish/Desktop/ams691/project_llm/data/\"\n",
    "\n",
    "# Get all PDF files in the directory\n",
    "pdf_files = [\n",
    "    os.path.join(data_directory, f)\n",
    "    for f in os.listdir(data_directory)\n",
    "    if f.lower().endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "all_docs = []\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    document = loader.load()\n",
    "    \n",
    "    # Get text chunks from the document using the defined get_chunks function\n",
    "    chunks = get_chunks(document, chunk_size=1000, chunk_overlap=200)\n",
    "    \n",
    "    # Process the document to get metadata for each chunk using the defined process_document function\n",
    "    docs = process_document(pdf_path, chunks)\n",
    "    all_docs.extend(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c34e8ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "{'message': 'Failed to send 10 in a batch of 192', 'errors': {'send POST request: Post \"http://host.docker.internal:11434/api/embed\": read tcp 172.18.0.2:41010->192.168.65.2:11434: read: connection reset by peer'}}\n",
      "{'message': 'Failed to send 10 objects in a batch of 192. Please inspect client.batch.failed_objects or collection.batch.failed_objects for the failed objects.'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed imports: 10\n",
      "First failed object: ErrorObject(message='send POST request: Post \"http://host.docker.internal:11434/api/embed\": read tcp 172.18.0.2:41010->192.168.65.2:11434: read: connection reset by peer', object_=BatchObject(collection='ResearchPapers', properties={'paper_title': 'TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING', 'doc_id': '4_chunk_95', 'chunk_text': '##sncj / bpi6ww2asshjaynjiemhgkkl4xgbssnjmybjl + [UNK] + wsstose4sstmq + ovpzl719 / [UNK] + kwujkoucoufco + eeocfr8jrz / ppoggqx + w8m2tmcoaavmcrvurqjnghsqhghephmldoxikdp9kkkyolvbeb1rc8rxscuvzrcuuyzgvuctvqytiokesiqjljrtdkzvqbgduwsu / pxiueo9kungxvmjfvqqslu4vlxbr + 1tcxbymei7idzvw8w6phit2ryvcra9uwkx6yo5x + a / [UNK] / bzowwunxwxackhdc4e2xdbwxabtfzdfhz8fmbhjygrenb1bec / vg04jx / pecckuminuorko9ofujatmg5i + eklqvahzjatd0olqtqpjhhurjkknfv3 + 7ncuk6b + z0tdblnmo237osnel / j53b2fe9njkb1lw2esbatx1waia + qrmnlkfaqg3tecchnfi6gib7pq976ugceezikjl9tl6 / 5uzvpzfrrodxsvkecmpu5jvww1aynw4kyxja5aag8dg2ekqivap1lzg2m / j /', 'tag': ''}, references=None, uuid='4950e8a2-72d8-5d07-81aa-44a10349f57c', vector=None, tenant=None, index=3720, retry_count=0), original_uuid='4950e8a2-72d8-5d07-81aa-44a10349f57c')\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.util import generate_uuid5\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# Get the collection \"ResearchPapers\"\n",
    "collection = client.collections.get(\"ResearchPapers\")\n",
    "\n",
    "with collection.batch.dynamic() as batch:\n",
    "    for doc in all_docs:   # each doc is a dictionary with keys (`paper_title`, `doc_id`, `chunk_text`, `tag`)\n",
    "        obj_uuid = generate_uuid5(doc)\n",
    "        batch.add_object(\n",
    "            properties=doc,\n",
    "            uuid=obj_uuid\n",
    "        )\n",
    "        if batch.number_errors > 10:\n",
    "            print(\"Batch import stopped due to excessive errors.\")\n",
    "            break\n",
    "\n",
    "failed_objects = collection.batch.failed_objects\n",
    "if failed_objects:\n",
    "    print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "    print(f\"First failed object: {failed_objects[0]}\")\n",
    "\n",
    "client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "d584c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def parse_query_return(query_return):\n",
    "    \"\"\"\n",
    "    Parses a QueryReturn-like object into a JSON string.\n",
    "    \"\"\"\n",
    "    parsed_objects = []\n",
    "\n",
    "    for obj in query_return.objects:\n",
    "        parsed_obj = {\n",
    "            \"uuid\": str(obj.uuid),\n",
    "            \"collection\": getattr(obj, \"collection\", None),\n",
    "            \"properties\": getattr(obj, \"properties\", {}),\n",
    "            \"metadata\": {\n",
    "                \"creation_time\": getattr(obj.metadata, \"creation_time\", None),\n",
    "                \"last_update_time\": getattr(obj.metadata, \"last_update_time\", None),\n",
    "                \"distance\": getattr(obj.metadata, \"distance\", None),\n",
    "                \"certainty\": getattr(obj.metadata, \"certainty\", None),\n",
    "                \"score\": getattr(obj.metadata, \"score\", None),\n",
    "                \"explain_score\": getattr(obj.metadata, \"explain_score\", None),\n",
    "                \"is_consistent\": getattr(obj.metadata, \"is_consistent\", None),\n",
    "                \"rerank_score\": getattr(obj.metadata, \"rerank_score\", None),\n",
    "            }\n",
    "        }\n",
    "        parsed_objects.append(parsed_obj)\n",
    "\n",
    "    return json.dumps(parsed_objects, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "f221c14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate, json\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "papers = client.collections.get(\"ResearchPapers\")\n",
    "\n",
    "response = papers.query.near_text(\n",
    "    query=\"What is LORA and how does it work?\",\n",
    "    distance=1,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True, distance=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "35d42244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"uuid\": \"76f41b71-9230-5d36-96c5-4ae3078f20a6\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"QLORA- Efficient Finetuning of Quantized LLMs\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"34_chunk_27\",\n",
      "      \"chunk_text\": \"##paca, we find that the most critical lora hyper - parameter is how many lora adapters are used in total and that lora on all linear transformer block layers are required to match full finetuning perfor - mance. other lora hyperparameters, such as the projection dimension r, do not affect performance ( see appendix a ). 1010 1011 t otal model bits 0. 60 0. 61 0. 62 0. 63 0. 64 0. 65 0. 66 0. 67mean zeroshot accuracy 4 - bit llama float nfloat nfloat + dq data type figure 3 : mean zero - shot accuracy over wino - grande, hellaswag, piqa, arc - easy, and arc - challenge using llama models with different 4 - bit data types. the normalfloat data type significantly improves the bit - for - bit accuracy gains compared to regular 4 - bit floats. while double quantization ( dq ) only leads to minor gains, it allows for a more fine - grained control over the memory footprint to fit models of certain size ( 33b / 65b ) into certain gpus ( 24 / 48gb ). similarly, we find that default hyperparameters for fully finetuned baselines are undertuned. we do a hyperparameter search over learning rates 1e - 6 to 5e - 5 and batch sizes 8 to 128 to find robust baselines. results for 7b llama finetuning on alpaca are shown in figure 2. 4 - bit normalfloat yields better performance than 4 - bit floating point while the 4 - bit normalfloat ( nf4 ) data type is information - theoretically optimal, it still needs to be determined if this property translates to empirical advantages. we follow the setup from dettmers and zettlemoyer [ 13 ] where\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3317769169807434,\n",
      "      \"certainty\": 0.8341115713119507,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"787597f8-30e2-5fd3-80d8-6f27a75b77d9\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"4_chunk_505\",\n",
      "      \"chunk_text\": \"##x tuning, and lora in eq. 4, 6, and 10 respectively. the functional forms of all these methods are similar with a proj down \\u2192 nonlinear \\u2192 proj up architecture, while \\u201c nonlinear \\u201d degenerates to the identity function in lora. modi\\ufb01ed representation indicates which hidden representation is directly modi\\ufb01ed. 8 insertion form is how the added module is inserted into the network. as mentioned in the previous section and shown in figure 3, traditionally adapters are inserted at a position in a sequential manner, where both the input and output are h. pre\\ufb01x tuning and lora \\u2013 although not originally described in this way \\u2013 turn out to be equivalent to a parallel insertion where x is the input. 7we will detail in \\u00a7 4. 1 the number of parameters added of different methods. 8strictly speaking, all the hidden representations would be indirectly in\\ufb02uenced by modifying the ones before them. here we refer to the position being directly modi\\ufb01ed by the added module. 5\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.33766937255859375,\n",
      "      \"certainty\": 0.8311653137207031,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"4106cd47-bc26-533d-be09-8f1e7caabbbc\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"QLORA- Efficient Finetuning of Quantized LLMs\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"34_chunk_102\",\n",
      "      \"chunk_text\": \"in figure 4 8 16 32 64 lora r 64. 0 64. 2 64. 4 64. 6 64. 8 65. 0rougel bits 4 figure 4 : lora r for llama 7b models finetuned on alpaca. each dot represents a combination of hyperparameters and for each lora r we run 3 random seed with each hyperparameter combination. the performance of specific lora r values appears to be independent of other hyperparameters. a. 2 super - natural instructions experimental setup details we use the same preprocessing of the super - natural instruction dataset as wang et al. [ 60 ]. however, we split the training data in training and validation datasets allowing us to perform more rigorous hyperparameter tuning and early stopping. we use the same hyperparameters described in the paper for training the various t5 model sizes on the super - natural instruction data. we use lora r = 16 for small, medium, and large t5 models and lora r = 64for t5 xl and xxl models. we also use lora \\u03b1 = 64in all our experiments and no lora dropout. b training a state - of - the - art chatbot experimental setup details b. 1 datasets we describe the datasets used for qlora finetuning experiments outlined in section 5. oasst1 the openassistant dataset [ 31 ] was collected via crowd - sourcing. it contains 161, 443 unique messages distributed across 66, 497 conversations and spanning 35 different languages. the dataset often contains several ranked replies for each given user question. in our experiments, we only use the top reply at each level in the conversation tree. this limits the dataset to 9, 209 examples. we finetuning our models on the full conversation including the user queries.\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.34495997428894043,\n",
      "      \"certainty\": 0.8275200128555298,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"326033be-a140-5000-b757-ef964a6fa0a2\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_6\",\n",
      "      \"chunk_text\": \") approach. lora allows us to train some dense layers in a neural network indirectly by optimizing rank decomposition matrices of the dense layers \\u2019 change during adaptation instead, while keeping the pre - trained weights frozen, as shown in figure 1. using gpt - 3 175b as an example, we show that a very low rank ( i. e., r in figure 1 can be one or two ) suf\\ufb01ces even when the full rank ( i. e., d ) is as high as 12, 288, making lora both storage - and compute - ef\\ufb01cient. lora possesses several key advantages. \\u2022 a pre - trained model can be shared and used to build many small lora modules for dif - ferent tasks. we can freeze the shared model and ef\\ufb01ciently switch tasks by replacing the matrices aand b in figure 1, reducing the storage requirement and task - switching over - head signi\\ufb01cantly. \\u2022 lora makes training more ef\\ufb01cient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. instead, we only optimize the injected, much smaller low - rank matrices. \\u2022 our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully \\ufb01ne - tuned model, by construction. \\u2022 lora is orthogonal to many prior methods and can be combined with many of them, such as pre\\ufb01x - tuning. we provide an example in appendix e. terminologies and conventions we make frequent references to the transformer architecture and use the conventional terminologies for its dimensions. we call the input and output di - mension size of a transformer layer dmodel. we use\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.348491907119751,\n",
      "      \"certainty\": 0.8257540464401245,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"f008d865-d649-51f4-896d-25589b5288a1\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_98\",\n",
      "      \"chunk_text\": \"##ra + pe and lora + pl on wikisql and multinli. first of all, lora + pe signi\\ufb01cantly outperforms both lora and pre\\ufb01x - embedding tuning on wikisql, which indicates that lora is somewhat orthogonal to pre\\ufb01x - embedding tuning. on multinli, the combination of lora + pe doesn \\u2019 t perform better than lora, possibly because lora on its own already achieves performance comparable to the human baseline. secondly, we notice that lora + pl performs slightly worse than lora even with more trainable parameters. we at - tribute this to the fact that pre\\ufb01x - layer tuning is very sensitive to the choice of learning rate and thus makes the optimization of lora weights more dif\\ufb01cult in lora + pl. f a dditional empirical experiments f. 1 a dditional experiments on gpt - 2 we also repeat our experiment on dart ( nan et al., 2020 ) and webnlg ( gardent et al., 2017 ) following the setup of li & liang ( 2021 ). the result is shown in table 13. similar to our result on e2e nlg challenge, reported in section 5, lora performs better than or at least on - par with pre\\ufb01x - based approaches given the same number of trainable parameters. method # trainable dart parameters bleu\\u2191 met\\u2191 ter\\u2193 gpt - 2 medium fine - tune 354m 46. 2 0. 39 0. 46 adapterl 0. 37m 42. 4 0. 36 0. 48 adapterl 11m 45. 2 0. 38 0. 46 fttop2 24m 41. 0 0. 34 0. 56 preflayer 0. 35m 46. 4 0. 38 0. 46 lora 0.\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3496066927909851,\n",
      "      \"certainty\": 0.8251966238021851,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"27d8839e-93bc-5702-b69c-eea4698f9ca4\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_22\",\n",
      "      \"chunk_text\": \"##tly fewer gpus and avoid i / o bottlenecks. another bene\\ufb01t is that we can switch between tasks while deployed at a much lower cost by only swapping the lora weights as opposed to all the parameters. this allows for the creation of many customized models that can be swapped in and out on the \\ufb02y on machines that store the pre - trained weights in vram. we also observe a 25 % speedup during training on gpt - 3 175b compared to full \\ufb01ne - tuning 5 as we do not need to calculate the gradient for the vast majority of the parameters. lora also has its limitations. for example, it is not straightforward to batch inputs to different tasks with differentaand bin a single forward pass, if one chooses to absorbaand binto w to eliminate additional inference latency. though it is possible to not merge the weights and dynamically choose the lora modules to use for samples in a batch for scenarios where latency is not critical. 5 e mpirical experiments we evaluate the downstream task performance of lora on roberta ( liu et al., 2019 ), de - berta ( he et al., 2021 ), and gpt - 2 ( radford et al., b ), before scaling up to gpt - 3 175b ( brown et al., 2020 ). our experiments cover a wide range of tasks, from natural language understanding ( nlu ) to generation ( nlg ). speci\\ufb01cally, we evaluate on the glue ( wang et al., 2019 ) benchmark for roberta and deberta. we follow the setup of li & liang ( 2021 ) on gpt - 2 for a direct com - parison and add wikisql ( zhong et al., 2017 ) ( nl to sql queries ) and samsum ( gliwa et al.,\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3532484769821167,\n",
      "      \"certainty\": 0.8233757615089417,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"d366dd72-b633-5711-8764-3725880e0f7d\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"QLORA- Efficient Finetuning of Quantized LLMs\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"34_chunk_101\",\n",
      "      \"chunk_text\": \"a qlora vs standard finetuning experimental setup details a. 1 hyperparameters for ql ora we do a hyperparameter search for lora over the following variables : lora dropout { 0. 0, 0. 05, 0. 1 }, lora r { 8, 16, 32, 64, 128, 256 }, lora layers { key + query, all attention layers, all ffn layers, all layers, attention + ffn output layers }. we keep lora \\u03b1 fixed and search the learning rate, since lora \\u03b1 is always proportional to the learning rate. we find that lora dropout 0. 05 is useful for small models ( 7b, 13b ), but not for larger models ( 33b, 65b ). we find lora r is unrelated to final performance if lora is used on all layers as can be seen in figure 4 8 16 32 64 lora r 64. 0 64. 2 64. 4 64. 6 64. 8 65. 0rougel bits 4 figure 4 : lora r for llama 7b models finetuned on alpaca. each dot represents a combination of hyperparameters and for each lora r we run 3 random seed with each hyperparameter combination. the performance of specific lora r values appears to be independent of other hyperparameters. a. 2 super - natural instructions experimental setup details we use the same preprocessing of the super - natural instruction dataset as wang et al. [ 60 ]. however, we split the training data in training and validation datasets allowing us to perform more rigorous hyperparameter tuning and early stopping. we use the same hyperparameters described in the paper for training the various t5 model sizes on the super - natural instruction data. we use lora r = 16 for small,\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3549492359161377,\n",
      "      \"certainty\": 0.8225253820419312,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"ff5a8e4e-2747-5d9e-8555-86eca5cece6d\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_3\",\n",
      "      \"chunk_text\": \". we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta, deberta, and gpt - 2 athttps : / / github. com / microsoft / lora. 1 i ntroduction pretrained weights [UNK] \\u2208 [UNK] x h [UNK] = 0 [UNK] = [UNK] ( 0, [UNK] ) [UNK] [UNK] pretrained weights [UNK] \\u2208 [UNK] x f ( x ) [UNK] figure 1 : our reparametriza - tion. we only train aand b. many applications in natural language processing rely on adapt - ing one large - scale, pre - trained language model to multiple down - stream applications. such adaptation is usually done via \\ufb01ne - tuning, which updates all the parameters of the pre - trained model. the ma - jor downside of \\ufb01ne - tuning is that the new model contains as many parameters as in the original model. as larger models are trained every few months, this changes from a mere \\u201c inconvenience \\u201d for gpt - 2 ( radford et al., b ) or roberta large ( liu et al., 2019 ) to a critical deployment challenge for gpt - 3 ( brown et al., 2020 ) with 175 billion trainable parameters. 1 many sought to mitigate this by adapting only some parameters or learning external modules for new tasks. this way, we only need to store and load a small number of task - speci\\ufb01c parameters in ad - dition to the pre - trained model for each task, greatly boosting the operational ef\\ufb01ciency when deployed. however, existing techniques \\u2217equal contribution. 0compared to v1, this draft includes better baselines, experiments on glue, and more on adapter latency. 1while gpt - 3 175b achieves non - trivial performance\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3609253168106079,\n",
      "      \"certainty\": 0.819537341594696,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"61a376d8-32e6-5522-91f6-2a278106944b\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"4_chunk_520\",\n",
      "      \"chunk_text\": \"##\\u00d7dm and w2 \\u2208rdm\\u00d7d. in this case wup in lora for w1 ( similar for wdown of w2 ) would have dimensions of r\\u00d7dm, where dm = 4das described in \\u00a7 2. 1. thus we typically use smaller r for lora ( ffn ) than other methods to match their overall parameter size in later experiments. results : as shown in figure 5, any method with ffn modi\\ufb01cation outperforms all the methods with attention modi\\ufb01cation in all cases ( the red markers are generally above all the blue ones, the only exception is ffn - pa with 2. 4 % params ), often with fewer parameters. second, the same method applied at ffn always improves over its attention counterpart. for example, lora ( ffn ) improves lora ( attn ) by 1 r - 2 points on xsum. we also highlight that pre\\ufb01x tuning does not keep improving when we further increase the capacity, which is also observed in li & liang ( 2021 ). these results suggest that ffn modi\\ufb01cation can utilize the added parameters more effectively than attention, no matter what the functional form or composition function is. we hypothesize that this is because the ffn learns task - speci\\ufb01c textual patterns ( geva et al., 2021 ), while attention learns pairwise positional interactions which do not require large capacity for adapting to new tasks. is the story different when we use 0. 1 % parameters? in \\u00a7 3. 1 we reason that pre\\ufb01x tuning is more expressive than adapters ( attn ), which, however, is not re\\ufb02ected in figure 5. we conjecture that this is because multi - head attention is only superior when the parameter budget is small. to validate this hypothesis, we compare pre\\ufb01x tuning\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.36208438873291016,\n",
      "      \"certainty\": 0.8189578056335449,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"4b017fa4-f4f8-56b4-94f2-8aa257d0d169\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_7\",\n",
      "      \"chunk_text\": \"requirement and task - switching over - head signi\\ufb01cantly. \\u2022 lora makes training more ef\\ufb01cient and lowers the hardware barrier to entry by up to 3 times when using adaptive optimizers since we do not need to calculate the gradients or maintain the optimizer states for most parameters. instead, we only optimize the injected, much smaller low - rank matrices. \\u2022 our simple linear design allows us to merge the trainable matrices with the frozen weights when deployed, introducing no inference latency compared to a fully \\ufb01ne - tuned model, by construction. \\u2022 lora is orthogonal to many prior methods and can be combined with many of them, such as pre\\ufb01x - tuning. we provide an example in appendix e. terminologies and conventions we make frequent references to the transformer architecture and use the conventional terminologies for its dimensions. we call the input and output di - mension size of a transformer layer dmodel. we use wq, wk, wv, and wo to refer to the query / key / value / output projection matrices in the self - attention module. w or w0 refers to a pre - trained weight matrix and \\u2206w its accumulated gradient update during adaptation. we use r to denote the rank of a lora module. we follow the conventions set out by ( vaswani et al., 2017 ; brown et al., 2020 ) and use adam ( loshchilov & hutter, 2019 ; kingma & ba, 2017 ) for model optimization and use a transformer mlp feedforward dimension dffn = 4 \\u00d7dmodel. 2 p roblem statement while our proposal is agnostic to training objective, we focus on language modeling as our motivat - ing use case. below is a brief description of the language modeling problem and, in particular, the maximization of\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3630090355873108,\n",
      "      \"certainty\": 0.818495512008667,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"be8df604-1981-5ecd-a28c-b7781937a377\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_1\",\n",
      "      \"chunk_text\": \"lora : l ow - rank adaptation of large lan - guage models edward hu\\u2217 yelong shen\\u2217 phillip wallis zeyuan allen - zhu yuanzhi li shean wang lu wang weizhu chen microsoft corporation { edwardhu, yeshe, phwallis, zeyuana, yuanzhil, swang, luw, wzchen } @ microsoft. com yuanzhil @ andrew. cmu. edu ( version 2 ) abstract an important paradigm of natural language processing consists of large - scale pre - training on general domain data and adaptation to particular tasks or domains. as we pre - train larger models, full \\ufb01ne - tuning, which retrains all model parameters, becomes less feasible. using gpt - 3 175b as an example \\u2013 deploying indepen - dent instances of \\ufb01ne - tuned models, each with 175b parameters, is prohibitively expensive. we propose low - rank adaptation, or lora, which freezes the pre - trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture, greatly reducing the number of trainable pa - rameters for downstream tasks. compared to gpt - 3 175b \\ufb01ne - tuned with adam, lora can reduce the number of trainable parameters by 10, 000 times and the gpu memory requirement by 3 times. lora performs on - par or better than \\ufb01ne - tuning in model quality on roberta, deberta, gpt - 2, and gpt - 3, despite hav - ing fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. we also provide an empirical investigation into rank - de\\ufb01ciency in language model adaptation, which sheds light on the ef\\ufb01cacy of lora. we release a package that facilitates the integration of lora with pytor\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.37080657482147217,\n",
      "      \"certainty\": 0.8145967125892639,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"bf21ba89-5951-5244-870e-81dbab6ed0d4\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_19\",\n",
      "      \"chunk_text\": \"tuning. a more general form of \\ufb01ne - tuning allows the training of a subset of the pre - trained parameters. lora takes a step further and does not require the accumu - lated gradient update to weight matrices to have full - rank during adaptation. this means that when applying lora to all weight matrices and training all biases 2, we roughly recover the expressive - ness of full \\ufb01ne - tuning by setting the lora rank rto the rank of the pre - trained weight matrices. in other words, as we increase the number of trainable parameters 3, training lora roughly converges to training the original model, while adapter - based methods converges to an mlp and pre\\ufb01x - based methods to a model that cannot take long input sequences. no additional inference latency. when deployed in production, we can explicitly compute and store w = w0 + ba and perform inference as usual. note that both w0 and ba are in rd\\u00d7k. when we need to switch to another downstream task, we can recover w0 by subtracting ba and then adding a different b \\u2032 a \\u2032, a quick operation with very little memory overhead. critically, this 2they represent a negligible number of parameters compared to weights. 3an inevitability when adapting to hard tasks. 4\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3736855387687683,\n",
      "      \"certainty\": 0.8131572008132935,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"f7d6a470-2f7a-51b7-a87e-96618dc4c308\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_91\",\n",
      "      \"chunk_text\": \"= rv = 8 lora \\u03b1 16 max seq. len. 128 128 512 128 512 512 512 512 roberta large lora \\u2020 batch size 4 # epochs 10 10 20 20 10 20 20 10 learning rate 3e - 04 4e - 04 3e - 04 2e - 04 2e - 04 3e - 04 4e - 04 2e - 04 lora con\\ufb01g. rq = rv = 8 lora \\u03b1 16 max seq. len. 128 roberta large adptp ( 3m ) \\u2020 batch size 32 # epochs 10 20 20 20 10 20 20 20 learning rate 3e - 05 3e - 05 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 bottleneck r 64 max seq. len. 128 roberta large adptp ( 0. 8m ) \\u2020 batch size 32 # epochs 5 20 20 20 10 20 20 20 learning rate 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 bottleneck r 16 max seq. len. 128 roberta large adpth ( 6m ) \\u2020 batch size 32 # epochs 10 5 10 10 5 20 20 10 learning rate 3e - 05 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 bottleneck r 64 max seq. len. 128 roberta large adpth ( 0. 8m ) \\u2020 batch size 32 # epochs 10 5 10 10 5 20 20 10 learning rate 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 bottleneck r 8 max seq. len. 128 table 9 : the hyperparameter\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3754388093948364,\n",
      "      \"certainty\": 0.8122805953025818,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"0bb52b8e-16f8-5fed-989c-7a2040688136\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_38\",\n",
      "      \"chunk_text\": \"match the performance of a fully \\ufb01ne - tuned deberta xxl ( 1. 5b ) on glue. the result is presented in table 2 ( bottom section ). see section d. 2 for details on the hyperparameters used. 5. 4 gpt - 2 medium / large having shown that lora can be a competitive alternative to full \\ufb01ne - tuning on nlu, we hope to answer if lora still prevails on nlg models, such as gpt - 2 medium and large ( radford et al., b ). we keep our setup as close as possible to li & liang ( 2021 ) for a direct comparison. due to space constraint, we only present our result on e2e nlg challenge ( table 3 ) in this section. see section f. 1 for results on webnlg ( gardent et al., 2017 ) and dart ( nan et al., 2020 ). we include a list of the hyperparameters used in section d. 3. 7\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.37593138217926025,\n",
      "      \"certainty\": 0.8120343089103699,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"6be30544-b302-55bf-a74b-427da221e221\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"4_chunk_7\",\n",
      "      \"chunk_text\": \"= = < / latexit > loralora lora + + pre\\ufb01x tuning feed forward add & layer norm adapter adapter : nonlinear adapter + < latexit sha1 _ base64 = \\\" 649t + crkukw6abujfmgcqkc9wpe = \\\" > aaa1pniclvtbd9vkdvbstemvtjlph / ucfdkrtpeszfkct1l5ii7uxaiksqiu9qgpc4kbiczchblcpbj2uc99bf5y / k32zgcw94cqz4rwsot5vj2duxwz + ymid / m4evlnm7 / 85kd / 97o / / 4ef / + if13 / 5t / / 8l7 / 65tf / eioyarhadzdfwxe39axeuqrxmpix3ouf + mkwhtvh / z7ib0sorjslftnp4vpih2k0jgjfinq3gcal2 + xn + 8 / fblzzeqn / vnwl7epiy6366x3 + 9xf / oxhlwtsbvaaxl8t3229y + wnhfzikyliud6yccj + 490p4hi9tpwhxaae7vpreijlyxlmb / 1lpaztxwpijepnkijgjlyeiysmwjft + kse / / 7si0nwqiq3mjcbt2jozp0bvjaicahnp8cipigj76gutv / adixo0vv5s / xhnn\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.37765514850616455,\n",
      "      \"certainty\": 0.8111724257469177,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"edad357c-81a2-5dae-be24-83fc14605a9f\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_2\",\n",
      "      \"chunk_text\": \"low - rank adaptation, or lora, which freezes the pre - trained model weights and injects trainable rank decomposition matrices into each layer of the transformer architecture, greatly reducing the number of trainable pa - rameters for downstream tasks. compared to gpt - 3 175b \\ufb01ne - tuned with adam, lora can reduce the number of trainable parameters by 10, 000 times and the gpu memory requirement by 3 times. lora performs on - par or better than \\ufb01ne - tuning in model quality on roberta, deberta, gpt - 2, and gpt - 3, despite hav - ing fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. we also provide an empirical investigation into rank - de\\ufb01ciency in language model adaptation, which sheds light on the ef\\ufb01cacy of lora. we release a package that facilitates the integration of lora with pytorch models and provide our implementations and model checkpoints for roberta, deberta, and gpt - 2 athttps : / / github. com / microsoft / lora. 1 i ntroduction pretrained weights [UNK] \\u2208 [UNK] x h [UNK] = 0 [UNK] = [UNK] ( 0, [UNK] ) [UNK] [UNK] pretrained weights [UNK] \\u2208 [UNK] x f ( x ) [UNK] figure 1 : our reparametriza - tion. we only train aand b. many applications in natural language processing rely on adapt - ing one large - scale, pre - trained language model to multiple down - stream applications. such adaptation is usually done via \\ufb01ne - tuning, which updates all the parameters of the pre - trained model. the ma - jor downside of \\ufb01ne - tuning is that the new model contains as many parameters as in the original model. as larger models are trained\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3845968246459961,\n",
      "      \"certainty\": 0.807701587677002,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"5e2237dc-5016-5d04-a5ea-3a4a2e656b94\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_95\",\n",
      "      \"chunk_text\": \"##berta xxl on tasks included in the glue benchmark. dataset e2e webnlg dart training optimizer adamw weight decay 0. 01 0. 01 0. 0 dropout prob 0. 1 0. 1 0. 0 batch size 8 # epoch 5 warmup steps 500 learning rate schedule linear label smooth 0. 1 0. 1 0. 0 learning rate 0. 0002 adaptation rq = rv = 4 lora \\u03b1 32 inference beam size 10 length penalty 0. 9 0. 8 0. 8 no repeat ngram size 4 table 11 : the hyperparameters for gpt - 2 lora on e2e, webnlg and dart. wikisql ( zhong et al., 2017 ), 768 for mnli ( williams et al., 2018 ), and 2048 for samsum ( gliwa et al., 2019 ). we tune learning rate for all method - dataset combinations. see section d. 4 for more details on the hyperparameters used. for pre\\ufb01x - embedding tuning, we \\ufb01nd the optimal lp and li to be 256 and 8, respectively, totalling 3. 2m trainable parameters. we use lp = 8 and li = 8 for pre\\ufb01x - layer tuning with 20. 2m trainable parameters to obtain the overall best performance. we present two parameter budgets for lora : 4. 7m ( rq = rv = 1 or rv = 2 ) and 37. 7m ( rq = rv = 8 or rq = rk = rv = ro = 2 ). we report the best validation performance from each run. the training hyperparameters used in our gpt - 3 experiments are listed in table 12. e c ombining lora with prefix tuning lora can be naturally combined with existing pre\\ufb01x - based approaches. in\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.38656091690063477,\n",
      "      \"certainty\": 0.8067195415496826,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"b25e66e8-471a-5af4-a4e5-867ceab9ad84\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_23\",\n",
      "      \"chunk_text\": \"##ra modules to use for samples in a batch for scenarios where latency is not critical. 5 e mpirical experiments we evaluate the downstream task performance of lora on roberta ( liu et al., 2019 ), de - berta ( he et al., 2021 ), and gpt - 2 ( radford et al., b ), before scaling up to gpt - 3 175b ( brown et al., 2020 ). our experiments cover a wide range of tasks, from natural language understanding ( nlu ) to generation ( nlg ). speci\\ufb01cally, we evaluate on the glue ( wang et al., 2019 ) benchmark for roberta and deberta. we follow the setup of li & liang ( 2021 ) on gpt - 2 for a direct com - parison and add wikisql ( zhong et al., 2017 ) ( nl to sql queries ) and samsum ( gliwa et al., 2019 ) ( conversation summarization ) for large - scale experiments on gpt - 3. see appendix c for more details on the datasets we use. we use nvidia tesla v100 for all experiments. 5. 1 b aselines to compare with other baselines broadly, we replicate the setups used by prior work and reuse their reported numbers whenever possible. this, however, means that some baselines might only appear in certain experiments. fine - tuning ( ft ) is a common approach for adaptation. during \\ufb01ne - tuning, the model is initialized to the pre - trained weights and biases, and all model parameters undergo gradient updates. a simple variant is to update only some layers while freezing others. we include one such baseline reported in prior work ( li & liang, 2021 ) on gpt - 2, which adapts just the last two layers ( fttop2 )\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3950918912887573,\n",
      "      \"certainty\": 0.8024540543556213,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"b249c349-fd1e-588d-88bc-1329d282d544\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_41\",\n",
      "      \"chunk_text\": \"and samsum around \\u00b10. 2 / \\u00b10. 2 / \\u00b10. 1 for the three metrics. 5. 5 s caling up to gpt - 3 175b as a \\ufb01nal stress test for lora, we scale up to gpt - 3 with 175 billion parameters. due to the high training cost, we only report the typical standard deviation for a given task over random seeds, as opposed to providing one for every entry. see section d. 4 for details on the hyperparameters used. as shown in table 4, lora matches or exceeds the \\ufb01ne - tuning baseline on all three datasets. note that not all methods bene\\ufb01t monotonically from having more trainable parameters, as shown in fig - ure 2. we observe a signi\\ufb01cant performance drop when we use more than 256 special tokens for pre\\ufb01x - embedding tuning or more than 32 special tokens for pre\\ufb01x - layer tuning. this corroborates similar observations in li & liang ( 2021 ). while a thorough investigation into this phenomenon is out - of - scope for this work, we suspect that having more special tokens causes the input distri - bution to shift further away from the pre - training data distribution. separately, we investigate the performance of different adaptation approaches in the low - data regime in section f. 3. 6 7 8 9 10 11 log10 # trainable parameters 0. 55 0. 60 0. 65 0. 70 0. 75validation accuracy wikisql method fine - tune prefixembed prefixlayer adapter ( h ) lora 6 7 8 9 10 11 log10 # trainable parameters 0. 84 0. 86 0. 88 0. 90 0. 92 multinli - matched figure 2 : gpt - 3 175b validation accuracy vs. number of train\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.39698368310928345,\n",
      "      \"certainty\": 0.8015081882476807,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"d1af6588-9fd2-54be-990d-be89cc6a44d9\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_61\",\n",
      "      \"chunk_text\": \"we include more top singular directions fromwq. this suggests that the low - rank adaptation matrix potentially ampli\\ufb01es the important features for speci\\ufb01c downstream tasks that were learned but not emphasized in the general pre - training model. 8 c onclusion and future work fine - tuning enormous language models is prohibitively expensive in terms of the hardware required and the storage / switching cost for hosting independent instances for different tasks. we propose lora, an ef\\ufb01cient adaptation strategy that neither introduces inference latency nor reduces input sequence length while retaining high model quality. importantly, it allows for quick task - switching when deployed as a service by sharing the vast majority of the model parameters. while we focused on transformer language models, the proposed principles are generally applicable to any neural networks with dense layers. there are many directions for future works. 1 ) lora can be combined with other ef\\ufb01cient adapta - tion methods, potentially providing orthogonal improvement. 2 ) the mechanism behind \\ufb01ne - tuning or lora is far from clear \\u2013 how are features learned during pre - training transformed to do well on downstream tasks? we believe that lora makes it more tractable to answer this than full \\ufb01ne - 12\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.40005624294281006,\n",
      "      \"certainty\": 0.799971878528595,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"82dffaf0-4a7a-5254-8bc0-5fab181ee203\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_90\",\n",
      "      \"chunk_text\": \"method dataset mnli sst - 2 mrpc cola qnli qqp rte sts - b optimizer adamw warmup ratio 0. 06 lr schedule linear roberta base lora batch size 16 16 16 32 32 16 32 16 # epochs 30 60 30 80 25 25 80 40 learning rate 5e - 04 5e - 04 4e - 04 4e - 04 4e - 04 5e - 04 5e - 04 4e - 04 lora con\\ufb01g. rq = rv = 8 lora \\u03b1 8 max seq. len. 512 roberta large lora batch size 4 4 4 4 4 4 8 8 # epochs 10 10 20 20 10 20 20 30 learning rate 3e - 04 4e - 04 3e - 04 2e - 04 2e - 04 3e - 04 4e - 04 2e - 04 lora con\\ufb01g. rq = rv = 8 lora \\u03b1 16 max seq. len. 128 128 512 128 512 512 512 512 roberta large lora \\u2020 batch size 4 # epochs 10 10 20 20 10 20 20 10 learning rate 3e - 04 4e - 04 3e - 04 2e - 04 2e - 04 3e - 04 4e - 04 2e - 04 lora con\\ufb01g. rq = rv = 8 lora \\u03b1 16 max seq. len. 128 roberta large adptp ( 3m ) \\u2020 batch size 32 # epochs 10 20 20 20 10 20 20 20 learning rate 3e - 05 3e - 05 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 3e - 04 bottleneck r 64 max seq. len. 128 roberta large adptp ( 0. 8m ) \\u2020 batch size 32 # epochs 5 20 20 20 10 20 20 20 learning rate 3e - 04 3e\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.4008776545524597,\n",
      "      \"certainty\": 0.7995611429214478,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"729bfbf3-89e9-5e9c-b08b-180322f49c7a\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_96\",\n",
      "      \"chunk_text\": \". we tune learning rate for all method - dataset combinations. see section d. 4 for more details on the hyperparameters used. for pre\\ufb01x - embedding tuning, we \\ufb01nd the optimal lp and li to be 256 and 8, respectively, totalling 3. 2m trainable parameters. we use lp = 8 and li = 8 for pre\\ufb01x - layer tuning with 20. 2m trainable parameters to obtain the overall best performance. we present two parameter budgets for lora : 4. 7m ( rq = rv = 1 or rv = 2 ) and 37. 7m ( rq = rv = 8 or rq = rk = rv = ro = 2 ). we report the best validation performance from each run. the training hyperparameters used in our gpt - 3 experiments are listed in table 12. e c ombining lora with prefix tuning lora can be naturally combined with existing pre\\ufb01x - based approaches. in this section, we evaluate two combinations of lora and variants of pre\\ufb01x - tuning on wikisql and mnli. lora + pre\\ufb01xembed ( lora + pe ) combines lora with pre\\ufb01x - embedding tuning, where we insert lp + li special tokens whose embeddings are treated as trainable parameters. for more on pre\\ufb01x - embedding tuning, see section 5. 1. lora + pre\\ufb01xlayer ( lora + pl ) combines lora with pre\\ufb01x - layer tuning. we also insert lp + li special tokens ; however, instead of letting the hidden representations of these tokens evolve natu - 20\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.40115106105804443,\n",
      "      \"certainty\": 0.7994244694709778,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"e47c940d-a50e-5cb0-aee5-919901b7d37e\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_53\",\n",
      "      \"chunk_text\": \"to our surprise, a rank as small as one suf\\ufb01ces for adapting bothwq and wv on these datasets while trainingwq alone needs a larger r. we conduct a similar experiment on gpt - 2 in section h. 2. table 6 shows that, surprisingly, lora already performs competitively with a very small r ( more so for { wq, wv } than just wq ). this suggests the update matrix \\u2206w could have a very small \\u201c intrinsic rank \\u201d. 6 to further support this \\ufb01nding, we check the overlap of the subspaces learned by different choices of r and by different random seeds. we argue that increasing r does not cover a more meaningful subspace, which suggests that a low - rank adaptation matrix is suf\\ufb01cient. 6however, we do not expect a small r to work for every task or dataset. consider the following thought experiment : if the downstream task were in a different language than the one used for pre - training, retraining the entire model ( similar to lora with r = dmodel ) could certainly outperform lora with a small r. 10\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.4032782316207886,\n",
      "      \"certainty\": 0.7983608841896057,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"d64669a0-4a9c-5b56-9b77-aacca2df00fe\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"LORA- LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"3_chunk_20\",\n",
      "      \"chunk_text\": \"guarantees that we do not introduce any additional latency during inference compared to a \\ufb01ne - tuned model by construction. 4. 2 a pplying lora to transformer in principle, we can apply lora to any subset of weight matrices in a neural network to reduce the number of trainable parameters. in the transformer architecture, there are four weight matrices in the self - attention module ( wq, wk, wv, wo ) and two in the mlp module. we treatwq ( or wk, wv ) as a single matrix of dimension dmodel\\u00d7dmodel, even though the output dimension is usually sliced into attention heads. we limit our study to only adapting the attention weights for downstream tasks and freeze the mlp modules ( so they are not trained in downstream tasks ) both for simplicity and parameter - ef\\ufb01ciency. we further study the effect on adapting different types of attention weight matrices in a transformer in section 7. 1. we leave the empirical investigation of adapting the mlp layers, layernorm layers, and biases to a future work. practical bene\\ufb01ts and limitations. the most signi\\ufb01cant bene\\ufb01t comes from the reduction in memory and storage usage. for a large transformer trained with adam, we reduce that vram usage by up to 2 / 3 if r [UNK] as we do not need to store the optimizer states for the frozen parameters. on gpt - 3 175b, we reduce the vram consumption during training from 1. 2tb to 350gb. with r = 4 and only the query and value projection matrices being adapted, the checkpoint size is reduced by roughly 10, 000 \\u00d7 ( from 350gb to 35mb ) 4. this allows us to train with signi\\ufb01 - cantly fewer gpus and avoid i / o bottlenecks. another bene\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.4042395353317261,\n",
      "      \"certainty\": 0.797880232334137,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"0c100f72-39a8-5ca3-b54f-ae691bda8bd8\",\n",
      "    \"collection\": \"ResearchPapers\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"TOWARDS A UNIFIED VIEW OF PARAMETER-EFFICIENT TRANSFER LEARNING\",\n",
      "      \"tag\": \"\",\n",
      "      \"doc_id\": \"4_chunk_551\",\n",
      "      \"chunk_text\": \"table 10, we show the number of parameters for one sub - layer. as we have explained in \\u00a7 4. 4, lora approximates the update of each weight matrix with a pair of wdown and wup, thus lora typically uses more parameters with the same ras other methods. finally, the total number of tunable parameters for pre\\ufb01x tuning, adapter variants and lora is | \\u03b8 | = | \\u03b8 | attn + | \\u03b8 | [UNK] as applicable. prompt tuning prepends l tunable vectors at the input layer and uses l\\u00d7dnumber of parameters. using mbart / bart as an example, we present the number of parameters used by several representative methods throughout our paper in table 11, where adapter variants include sequential adapter, parallel adapter, scaled adapter and multi - head adapter. table 11 : number of tunable parameters of various parameter - ef\\ufb01cient tuning methods with bart / mbart models ( l = 12 ) as an example. method number of parameters prompt tuning l\\u00d7d pre\\ufb01x tuning ( attn ) 2ld\\u00d73 \\u00d712 adapter variants ( attn ) 2rd\\u00d73 \\u00d712 adapter variants ( ffn ) 2rd\\u00d72 \\u00d712 lora ( attn ) 4rd\\u00d73 \\u00d712 lora ( ffn ) 10rd\\u00d72 \\u00d712 mam adapter ( our proposed model ) 2ld\\u00d73 \\u00d712 + 2rd\\u00d72 \\u00d712 c f ull results on different bottleneck dimensions 14\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.4051637053489685,\n",
      "      \"certainty\": 0.7974181175231934,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_output = parse_query_return(response)\n",
    "print(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4c7ddab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Final Answer:\n",
      " LoRA (Low-Rank Adaptation) is a method used in transformer-based language models to reduce the computational cost of fine-tuning large models on specific tasks. It works by adapting a low-rank matrix that represents the relationships between input and output embeddings, rather than fine-tuning the entire model.\n",
      "\n",
      "In traditional fine-tuning methods, the entire model is updated based on the task-specific dataset, which can be computationally expensive. LoRA reduces this cost by using a separate adapter layer to adapt the input/output embeddings in a low-rank matrix format. This allows for efficient transfer learning and improved performance on specific tasks.\n",
      "\n",
      "The key idea behind LoRA is that the relationships between input and output embeddings are largely learned during pre-training, and can be adapted to the specific task using a smaller number of parameters. By using a low-rank adaptation, LoRA reduces the number of parameters required for fine-tuning, making it more computationally efficient.\n",
      "\n",
      "The LoRA architecture typically consists of an adapter layer that is inserted into the transformer block layers. The adapter layer learns a low-rank matrix that represents the relationships between input and output embeddings, which are then applied to the input/output embeddings using a projection-down and projection-up architecture. This allows for efficient adaptation of the model to the specific task.\n",
      "\n",
      "Overall, LoRA provides a computationally efficient method for fine-tuning large language models on specific tasks, while maintaining high performance levels.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "# Reconnect to client\n",
    "client = weaviate.connect_to_local(skip_init_checks=True)\n",
    "papers = client.collections.get(\"ResearchPapers\")\n",
    "\n",
    "# User question\n",
    "question = \"What is LoRA and how does it work?\"\n",
    "\n",
    "# Generate an answer using the most relevant chunks\n",
    "response = papers.generate.near_text(\n",
    "    query=question,\n",
    "    distance=0.8,\n",
    "    limit=2,\n",
    "    return_metadata=MetadataQuery(distance=True),  # optional\n",
    "    single_prompt=f\"Answer this question using the given context: {question}\\nContext: {{chunk_text}}\",\n",
    "    grouped_task=f\"Based on the following context, answer the question: {question}\"\n",
    ")\n",
    "\n",
    "# View generated summary answer\n",
    "print(\"🧠 Final Answer:\\n\", response.generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c545ce46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Final Answer:\n",
      " Based on the provided context, scaling laws in LLMs (Large Language Models) refer to empirical regularities that describe how the performance of a language model improves with increasing model size and computational resources.\n",
      "\n",
      "In this context, scaling laws are mathematically represented by equation (1.1), which describes the relationship between the number of parameters (n), the compute scaling factor (c), and the loss in nats/token. The equation is:\n",
      "\n",
      "loss = 0.28 + (n-1e4) -0.16 + (c-1.4e-5) -0.17(c2.3e-12)\n",
      "\n",
      "The paper discusses the implications of these scaling laws, including their potential to unlock new capabilities in machine learning models.\n",
      "\n",
      "Scaling laws are seen as a shift in perspective away from focusing on specific neural architectures, loss functions, and training algorithms towards identifying broader commonalities that appear when studying machine learning across different scales. They suggest that many capabilities may lie on a spectrum that can be continuously unlocked with increasing scale, as demonstrated by the metalearning capabilities of the GPT-3 model.\n",
      "\n",
      "The paper also mentions the identification of entropy in the true data distribution and the KL divergence between the true data distribution and a given generative model, which are two terms in equation (1.1). These terms can be interpreted as the entropy of the true data distribution and the KL divergence, respectively.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "# Reconnect to client\n",
    "client = weaviate.connect_to_local(skip_init_checks=True)\n",
    "papers = client.collections.get(\"ResearchPapers\")\n",
    "\n",
    "# User question\n",
    "question = \"What are scaling laws in LLMs?\"\n",
    "\n",
    "# Generate an answer using the most relevant chunks\n",
    "response = papers.generate.near_text(\n",
    "    query=question,\n",
    "    distance=0.8,\n",
    "    limit=2,\n",
    "    return_metadata=MetadataQuery(distance=True),  # optional\n",
    "    single_prompt=f\"Answer this question using the given context: {question}\\nContext: {{chunk_text}}\",\n",
    "    grouped_task=f\"Based on the following context, answer the question: {question}\"\n",
    ")\n",
    "\n",
    "# View generated summary answer\n",
    "print(\"🧠 Final Answer:\\n\", response.generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a478073",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
