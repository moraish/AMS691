{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d93ff84",
   "metadata": {},
   "source": [
    "## 1. Create Question Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2bda36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'questions' created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/weaviate/collections/classes/config.py:1950: PydanticDeprecatedSince211: Accessing the 'model_fields' attribute on the instance is deprecated. Instead, you should access this attribute from the model class. Deprecated in Pydantic V2.11 to be removed in V3.0.\n",
      "  for cls_field in self.model_fields:\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "\n",
    "# client.collections.delete(\"ResearchPapers\")  # THIS WILL DELETE THE SPECIFIED COLLECTION(S) AND THEIR OBJECTS\n",
    "\n",
    "\n",
    "try:\n",
    "    questions = client.collections.create(\n",
    "        name=\"questions\",\n",
    "        vectorizer_config=Configure.Vectorizer.text2vec_ollama(     # Configure the Ollama embedding integration\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "            model=\"nomic-embed-text\",                               # The model to use\n",
    "        ),\n",
    "        generative_config=Configure.Generative.ollama(              # Configure the Ollama generative integration\n",
    "            api_endpoint=\"http://host.docker.internal:11434\",       # Allow Weaviate from within a Docker container to contact your Ollama instance\n",
    "            model=\"llama3.2\",                                       # The model to use\n",
    "        ),\n",
    "        properties=[\n",
    "            Property(name=\"paper_title\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "            Property(name=\"doc_id\", data_type=DataType.TEXT, skip_vectorization=True),\n",
    "            Property(name=\"question_text\", data_type=DataType.TEXT, skip_vectorization=False)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(\"Collection 'questions' created successfully.\")\n",
    "except weaviate.exceptions.WeaviateQueryError as e:\n",
    "    print(f\"Error creating collection: {e}\")\n",
    "    # Optionally, handle the error or exit\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "    # Handle other exceptions\n",
    "finally:\n",
    "    client.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c2bb2",
   "metadata": {},
   "source": [
    "# 2. Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708422b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/moraish/Desktop/ams691/project_llm/questions.csv\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f16695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_Prefix-Tuning- Optimizing Continuous Prompts...</td>\n",
       "      <td>What is the key difference between prefix-tuni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_Prefix-Tuning- Optimizing Continuous Prompts...</td>\n",
       "      <td>In the context of table-to-text generation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_Prefix-Tuning- Optimizing Continuous Prompts...</td>\n",
       "      <td>What are the observed benefits of prefix-t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_Prefix-Tuning- Optimizing Continuous Prompts...</td>\n",
       "      <td>How does prefix-tuning perform in extrapol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_Prefix-Tuning- Optimizing Continuous Prompts...</td>\n",
       "      <td>How does the choice of prefix length impac...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  1_Prefix-Tuning- Optimizing Continuous Prompts...   \n",
       "1  1_Prefix-Tuning- Optimizing Continuous Prompts...   \n",
       "2  1_Prefix-Tuning- Optimizing Continuous Prompts...   \n",
       "3  1_Prefix-Tuning- Optimizing Continuous Prompts...   \n",
       "4  1_Prefix-Tuning- Optimizing Continuous Prompts...   \n",
       "\n",
       "                                                   1  \n",
       "0  What is the key difference between prefix-tuni...  \n",
       "1      In the context of table-to-text generation...  \n",
       "2      What are the observed benefits of prefix-t...  \n",
       "3      How does prefix-tuning perform in extrapol...  \n",
       "4      How does the choice of prefix length impac...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2ee6542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed 895 rows from CSV.\n",
      "Connected to Weaviate.\n",
      "Accessed 'questions' collection.\n",
      "Starting batch import of 895 questions...\n",
      "All objects imported successfully.\n",
      "Weaviate client closed.\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "import pandas as pd\n",
    "from weaviate.util import generate_uuid5\n",
    "import traceback\n",
    "\n",
    "# --- 1. Read and Process CSV Data ---\n",
    "try:\n",
    "    df = pd.read_csv(\"/Users/moraish/Desktop/ams691/project_llm/questions.csv\", header=None)\n",
    "    all_questions = []\n",
    "    processed_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            col_0 = str(row[0]) # Ensure it's a string\n",
    "            col_1 = str(row[1]).strip() # Ensure it's a string and remove leading/trailing whitespace\n",
    "\n",
    "            # Skip rows with empty question text\n",
    "            if not col_1:\n",
    "                print(f\"Skipping row {index + 1}: Empty question text.\")\n",
    "                error_count += 1\n",
    "                continue\n",
    "\n",
    "            # Split column 0 to get doc_id and paper_title\n",
    "            parts = col_0.split(\"_\", 1)\n",
    "            if len(parts) == 2:\n",
    "                doc_id_str = parts[0]\n",
    "                paper_title_str = parts[1]\n",
    "\n",
    "                # Prepare the data object for Weaviate\n",
    "                question_obj = {\n",
    "                    \"paper_title\": paper_title_str,\n",
    "                    \"doc_id\": doc_id_str,\n",
    "                    \"question_text\": col_1\n",
    "                }\n",
    "                all_questions.append(question_obj)\n",
    "                processed_count += 1\n",
    "            else:\n",
    "                print(f\"Skipping row {index + 1}: Column 0 format incorrect ('{col_0}'). Expected 'id_papername'.\")\n",
    "                error_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index + 1}: {e}\")\n",
    "            error_count += 1\n",
    "\n",
    "    print(f\"Successfully processed {processed_count} rows from CSV.\")\n",
    "    if error_count > 0:\n",
    "        print(f\"Skipped {error_count} rows due to errors or empty questions.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: questions.csv not found at the specified path.\")\n",
    "    all_questions = [] # Ensure list is empty if file not found\n",
    "except Exception as e:\n",
    "    print(f\"Error reading or processing CSV: {e}\")\n",
    "    all_questions = [] # Ensure list is empty on other errors\n",
    "\n",
    "# --- 2. Ingest Data into Weaviate ---\n",
    "if all_questions: # Proceed only if there are questions to ingest\n",
    "    client = None # Initialize client to None\n",
    "    try:\n",
    "        client = weaviate.connect_to_local()\n",
    "        print(\"Connected to Weaviate.\")\n",
    "\n",
    "        # Get the collection \"questions\"\n",
    "        questions_collection = client.collections.get(\"questions\")\n",
    "        print(\"Accessed 'questions' collection.\")\n",
    "\n",
    "        print(f\"Starting batch import of {len(all_questions)} questions...\")\n",
    "        with questions_collection.batch.dynamic() as batch:\n",
    "            for question_data in all_questions:\n",
    "                try:\n",
    "                    # Generate a consistent UUID based on the question data\n",
    "                    obj_uuid = generate_uuid5(question_data)\n",
    "                    batch.add_object(\n",
    "                        properties=question_data,\n",
    "                        uuid=obj_uuid\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Error adding object to batch: {question_data}. Error: {e}\")\n",
    "                    # Optionally break or continue based on error tolerance\n",
    "                    # if batch.number_errors > 10:\n",
    "                    #     print(\"Stopping batch due to excessive errors.\")\n",
    "                    #     break\n",
    "\n",
    "        # print(f\"Batch import finished. Added: {batch.number_imported}, Errors: {batch.number_errors}\")\n",
    "\n",
    "        # Check for failed objects specifically\n",
    "        failed_objects = questions_collection.batch.failed_objects\n",
    "        if failed_objects:\n",
    "            print(f\"Number of failed imports: {len(failed_objects)}\")\n",
    "            # Print details of the first few failed objects for debugging\n",
    "            for i, failed in enumerate(failed_objects[:5]):\n",
    "                 print(f\"  Failed object {i+1}: {failed}\")\n",
    "        else:\n",
    "             print(\"All objects imported successfully.\")\n",
    "\n",
    "\n",
    "    except weaviate.exceptions.WeaviateQueryError as e:\n",
    "        print(f\"Weaviate Query Error during ingestion: {e}\")\n",
    "        traceback.print_exc()\n",
    "    except weaviate.exceptions.WeaviateStartUpError as e:\n",
    "        print(f\"Weaviate Connection Error: {e}. Is Weaviate running and accessible?\")\n",
    "        traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during Weaviate ingestion: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        if client:\n",
    "            client.close()\n",
    "            print(\"Weaviate client closed.\")\n",
    "else:\n",
    "    print(\"No questions processed from CSV to ingest.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d0c0da",
   "metadata": {},
   "source": [
    "## Helper function to convert to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b07d53ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_query_return(query_return):\n",
    "    \"\"\"\n",
    "    Parses a QueryReturn-like object into a JSON string.\n",
    "    \"\"\"\n",
    "    parsed_objects = []\n",
    "\n",
    "    for obj in query_return.objects:\n",
    "        parsed_obj = {\n",
    "            \"uuid\": str(obj.uuid),\n",
    "            \"collection\": getattr(obj, \"collection\", None),\n",
    "            \"properties\": getattr(obj, \"properties\", {}),\n",
    "            \"metadata\": {\n",
    "                \"creation_time\": getattr(obj.metadata, \"creation_time\", None),\n",
    "                \"last_update_time\": getattr(obj.metadata, \"last_update_time\", None),\n",
    "                \"distance\": getattr(obj.metadata, \"distance\", None),\n",
    "                \"certainty\": getattr(obj.metadata, \"certainty\", None),\n",
    "                \"score\": getattr(obj.metadata, \"score\", None),\n",
    "                \"explain_score\": getattr(obj.metadata, \"explain_score\", None),\n",
    "                \"is_consistent\": getattr(obj.metadata, \"is_consistent\", None),\n",
    "                \"rerank_score\": getattr(obj.metadata, \"rerank_score\", None),\n",
    "            }\n",
    "        }\n",
    "        parsed_objects.append(parsed_obj)\n",
    "\n",
    "    return json.dumps(parsed_objects, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "595ddf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QueryReturn(objects=[Object(uuid=_WeaviateUUIDInt('eee90964-802a-51e9-a2fe-d414ead67e9d'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.14223623275756836, certainty=0.9288818836212158, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'paper_title': 'Documenting Large Webtext Corpora- A Case Study on the Colossal Clean Crawled Corpus', 'question_text': 'What are the key issues identified in the data provenance analysis for C4?', 'doc_id': '40'}, references=None, vector={}, collection='Questions'), Object(uuid=_WeaviateUUIDInt('750a3aba-8a99-5e92-98ea-b172b3daa925'), metadata=MetadataReturn(creation_time=None, last_update_time=None, distance=0.3534918427467346, certainty=0.8232541084289551, score=None, explain_score=None, is_consistent=None, rerank_score=None), properties={'paper_title': 'Documenting Large Webtext Corpora- A Case Study on the Colossal Clean Crawled Corpus', 'question_text': 'What challenges are associated with the lack of documentation in large web-crawled datasets like C4?', 'doc_id': '40'}, references=None, vector={}, collection='Questions')])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/3b88347n3ts9zy55rbjcw_b80000gn/T/ipykernel_30622/621102687.py:5: ResourceWarning: unclosed <socket.socket fd=79, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56094, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  questions = client.collections.get('questions')\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "client = weaviate.connect_to_local()\n",
    "\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "questions = client.collections.get('questions')\n",
    "\n",
    "question = \"What are the key issues identified in the data provenance?\"\n",
    "\n",
    "response = questions.query.near_text(\n",
    "    query=question,\n",
    "    distance=0.8,\n",
    "    limit=2,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True, distance=True)\n",
    "\n",
    ")\n",
    "\n",
    "print(response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "803545d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"uuid\": \"eee90964-802a-51e9-a2fe-d414ead67e9d\",\n",
      "    \"collection\": \"Questions\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"Documenting Large Webtext Corpora- A Case Study on the Colossal Clean Crawled Corpus\",\n",
      "      \"question_text\": \"What are the key issues identified in the data provenance analysis for C4?\",\n",
      "      \"doc_id\": \"40\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.14223623275756836,\n",
      "      \"certainty\": 0.9288818836212158,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"uuid\": \"750a3aba-8a99-5e92-98ea-b172b3daa925\",\n",
      "    \"collection\": \"Questions\",\n",
      "    \"properties\": {\n",
      "      \"paper_title\": \"Documenting Large Webtext Corpora- A Case Study on the Colossal Clean Crawled Corpus\",\n",
      "      \"question_text\": \"What challenges are associated with the lack of documentation in large web-crawled datasets like C4?\",\n",
      "      \"doc_id\": \"40\"\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"creation_time\": null,\n",
      "      \"last_update_time\": null,\n",
      "      \"distance\": 0.3534918427467346,\n",
      "      \"certainty\": 0.8232541084289551,\n",
      "      \"score\": null,\n",
      "      \"explain_score\": null,\n",
      "      \"is_consistent\": null,\n",
      "      \"rerank_score\": null\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "json_output = parse_query_return(response)\n",
    "print(json_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7ab162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
