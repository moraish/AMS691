{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21cf3511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moraish/Desktop/ams691/project_llm/.venv/lib/python3.9/site-packages/weaviate/warnings.py:314: ResourceWarning: Con004: The connection to Weaviate was not closed properly. This can lead to memory leaks.\n",
      "            Please make sure to close the connection using `client.close()`.\n",
      "  warnings.warn(\n",
      "/var/folders/86/3b88347n3ts9zy55rbjcw_b80000gn/T/ipykernel_52957/722657268.py:8: ResourceWarning: unclosed <socket.socket fd=76, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=6, laddr=('::1', 56859, 0, 0), raddr=('::1', 8080, 0, 0)>\n",
      "  client = weaviate.connect_to_local(\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.config import Configure, Property, DataType\n",
    "import weaviate.classes as wvc\n",
    "\n",
    "additional_config = wvc.init.AdditionalConfig(\n",
    "        timeout=wvc.init.Timeout(init=4, query=600, insert=60) # Query timeout set to 120 seconds (2 minutes)\n",
    ")\n",
    "client = weaviate.connect_to_local(\n",
    "        additional_config=additional_config,\n",
    "        skip_init_checks=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3206e23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bee7332",
   "metadata": {},
   "source": [
    "## Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a140e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def parse_query_return(query_return):\n",
    "    \"\"\"\n",
    "    Parses a QueryReturn-like object into a JSON string.\n",
    "    \"\"\"\n",
    "    parsed_objects = []\n",
    "\n",
    "    for obj in query_return.objects:\n",
    "        parsed_obj = {\n",
    "            \"uuid\": str(obj.uuid),\n",
    "            \"collection\": getattr(obj, \"collection\", None),\n",
    "            \"properties\": getattr(obj, \"properties\", {}),\n",
    "            \"metadata\": {\n",
    "                \"creation_time\": getattr(obj.metadata, \"creation_time\", None),\n",
    "                \"last_update_time\": getattr(obj.metadata, \"last_update_time\", None),\n",
    "                \"distance\": getattr(obj.metadata, \"distance\", None),\n",
    "                \"certainty\": getattr(obj.metadata, \"certainty\", None),\n",
    "                \"score\": getattr(obj.metadata, \"score\", None),\n",
    "                \"explain_score\": getattr(obj.metadata, \"explain_score\", None),\n",
    "                \"is_consistent\": getattr(obj.metadata, \"is_consistent\", None),\n",
    "                \"rerank_score\": getattr(obj.metadata, \"rerank_score\", None),\n",
    "            }\n",
    "        }\n",
    "        parsed_objects.append(parsed_obj)\n",
    "\n",
    "    return json.dumps(parsed_objects, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c72798ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_citations(query_return_json):\n",
    "    citations = set()\n",
    "    try:\n",
    "        # Parse the JSON string into a Python list of dictionaries\n",
    "        query_results = json.loads(query_return_json)\n",
    "\n",
    "        if not isinstance(query_results, list):\n",
    "            print(\"Error: Input is not a valid list of results.\")\n",
    "            return citations\n",
    "\n",
    "        for obj in query_results:\n",
    "            try:\n",
    "                # Extract required fields\n",
    "                doc_id = obj.get('properties', {}).get('doc_id')\n",
    "                paper_title = obj.get('properties', {}).get('paper_title')\n",
    "                certainty = obj.get('metadata', {}).get('certainty')\n",
    "\n",
    "                # Add to set only if all required fields are present\n",
    "                if doc_id is not None and paper_title is not None and certainty is not None:\n",
    "                    citations.add((doc_id, paper_title, certainty))\n",
    "                else:\n",
    "                    print(f\"Warning: Skipping object due to missing data: {obj.get('uuid', 'UUID N/A')}\")\n",
    "\n",
    "            except KeyError as e:\n",
    "                print(f\"Warning: Skipping object due to missing key: {e}. Object: {obj.get('uuid', 'UUID N/A')}\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: An error occurred processing object {obj.get('uuid', 'UUID N/A')}: {e}\")\n",
    "\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Invalid JSON input string.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred in get_citations: {e}\")\n",
    "\n",
    "    return citations    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2679eae",
   "metadata": {},
   "source": [
    "## Loading collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6aa6051d",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = client.collections.get('questions')\n",
    "papers = client.collections.get(\"ResearchPapers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825e9a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt =  \"What distinguishes GPT-3 from previous language models in terms of few-shot learning?\" # ENTER THE CUSTOM PROMPT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa381e4",
   "metadata": {},
   "source": [
    "## Question Similarity Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de42acbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_que_similarity = questions.query.near_text(\n",
    "    query=user_prompt,\n",
    "    distance=0.8,\n",
    "    limit=5,\n",
    "    return_metadata=wvc.query.MetadataQuery(certainty=True, distance=True)\n",
    ")\n",
    "\n",
    "\n",
    "docs_que_similarity = parse_query_return(docs_que_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "80480354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('16', 'Language Models are Few-Shot Learners', 0.9278692007064819)\n",
      "('16', 'Language Models are Few-Shot Learners', 0.9159530401229858)\n",
      "('16', 'Language Models are Few-Shot Learners', 0.9314121007919312)\n",
      "('16', 'Language Models are Few-Shot Learners', 0.9199385046958923)\n",
      "('16', 'Language Models are Few-Shot Learners', 0.9943397641181946)\n"
     ]
    }
   ],
   "source": [
    "citations = get_citations(docs_que_similarity)\n",
    "for citation in citations:\n",
    "    print(citation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b47ad",
   "metadata": {},
   "source": [
    "## Paper Similarity Score\n",
    "\n",
    "- Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7827dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INSTRUCTDIAL is a dataset used for training and testing dialogue models, specifically designed to improve zero- and few-shot generalization in dialogue. It consists of tasks created from existing open-access dialogue datasets, categorized into various types such as classification, generation, evaluation, edit, pretraining, safety, and miscellaneous tasks.\n"
     ]
    }
   ],
   "source": [
    "doc_search = papers.generate.hybrid(\n",
    "    query=user_prompt,\n",
    "    limit=1,\n",
    "    # distance=0.8,\n",
    "    # fusion_type=\"relativeScoreFusion\",\n",
    "    alpha=0.7,\n",
    "    return_metadata=wvc.query.MetadataQuery(distance=True), \n",
    "    single_prompt=f\"Answer this question using the given context: {user_prompt}\\nContext: {{chunk_text}}\",\n",
    "    grouped_task=f\"Based on the following context, answer the question: {user_prompt}\"\n",
    ")\n",
    "\n",
    "print(doc_search.generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae9133fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of speculative sampling is to accelerate large language model decoding by generating multiple tokens from each transformer call, allowing for a faster and more efficient decoding process. By leveraging the latency of parallel scoring of short continuations generated by a faster but less powerful draft model, speculative sampling can achieve significant speedups without compromising on sample quality or requiring modifications to the target model itself.\n",
      "\n",
      "In summary, speculative sampling enables the generation of multiple tokens from each transformer call, which allows for:\n",
      "\n",
      "1. Faster decoding: By generating multiple tokens simultaneously, the total decoding time is reduced.\n",
      "2. Reduced communication overheads: Serving a powerful draft model on the same hardware as the target model reduces the need for expensive communication operations.\n",
      "3. Improved efficiency: Speculative sampling enables the use of a faster but less powerful draft model to generate short continuations, which can be parallelized to reduce latency.\n",
      "\n",
      "Overall, speculative sampling is a novel approach that has shown promising results in accelerating large language model decoding without compromising on sample quality or requiring modifications to the target model itself.\n"
     ]
    }
   ],
   "source": [
    "doc_search = papers.generate.hybrid(\n",
    "    query=user_prompt,\n",
    "    limit=5,\n",
    "    # distance=0.8,\n",
    "    # fusion_type=\"relativeScoreFusion\",\n",
    "    alpha=0,\n",
    "    return_metadata=wvc.query.MetadataQuery(distance=True), \n",
    "    single_prompt=f\"Answer this question using the given context: {user_prompt}\\nContext: {{chunk_text}}\",\n",
    "    grouped_task=f\"Based on the following context, answer the question: {user_prompt}\"\n",
    ")\n",
    "\n",
    "print(doc_search.generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5210798",
   "metadata": {},
   "source": [
    "## Proposed Flow \n",
    "\n",
    "- User enters a prompt \n",
    "- This prompt get embedded [nomic-embed-text] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Search for similarity with question  \n",
    "\n",
    "- We get a certainity score for the top 5 results\n",
    "\n",
    "- If certanity > 0.85, then pull that paper into context, and generate\n",
    "\n",
    "- Else run search on all the docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e14de",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"\"\n",
    "def run_rag(user_query):\n",
    "\n",
    "\n",
    "    return ans\n",
    "\n",
    "print(run_rag(user_query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9e4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentence Transformer - \n",
    "    Tokens - 1000\n",
    "    Overlap - 200\n",
    "\n",
    "\n",
    "Evals \n",
    "1. Recursive Transformer for text splitting - did not use because files are not in MD format. \n",
    "\n",
    "2. Token Window / Overlap window for Sentence Transformer - [Need to do this in the future].\n",
    "\n",
    "3. nomic-embed-text - OpenAI - better than openai opensource model\n",
    "        Lightweight 137M params\n",
    "        Used to embed - questions, research papers, and user prompt. \n",
    "\n",
    "4. Llama3.2 [1.3B param model] - light weight, opensource, and allowed us to run on local \n",
    "    - We would get a giant performance leap, if we use an API for a better model. \n",
    "\n",
    "5. How did we tune alpha - experimented with 0.5,0.6, 0.7. 0.7 gave us the best results. But we still need to experminet more. [We checked with only 100 retrievals]\n",
    "\n",
    "6. We are currently only working on evaluating retrieval performance. We need to device a method for evaluating the answers. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
